{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39d13366-5880-4c68-be0a-3a241e13cb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and ready for inference\n",
      "Predicted emotion: happy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa as lb\n",
    "import numpy as np\n",
    "\n",
    "#I am using it in jupyter so I defined DNN again\n",
    "#not recommanded using it in jupyter\n",
    "#If you use vscode or pycharm, just'from train import DNN'\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, input_size=40, hidden_size=128, output_size=8):\n",
    "        super(DNN, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.output = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    \n",
    "#Initialize the model\n",
    "model = DNN().to(\"cuda\")\n",
    "\n",
    "#Load the saved model\n",
    "model.load_state_dict(torch.load('/path/to/your/emotion_model.pth'))\n",
    "model.eval()  #set the model to evaluation mode\n",
    "\n",
    "print(\"Model loaded and ready for inference\")\n",
    "\n",
    "#Function of extract audio features, it's same as in get_data.ipynb\n",
    "def audio_features(wav_file_path, mfcc=True, chroma=False, mel=False, sample_rate=22050):\n",
    "    audio, sample_rate = lb.load(wav_file_path, sr=sample_rate)\n",
    "    if len(audio.shape) != 1:\n",
    "        return None\n",
    "    result = np.array([])\n",
    "    if mfcc:\n",
    "        mfccs = np.mean(lb.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result = np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        stft = np.abs(lb.stft(audio))\n",
    "        chroma = np.mean(lb.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, chroma))\n",
    "    if mel:\n",
    "        mel = np.mean(lb.feature.melspectrogram(y=audio, sr=sample_rate, n_mels=40, fmin=0, fmax=sample_rate//2).T, axis=0)\n",
    "        result = np.hstack((result, mel))\n",
    "    return result\n",
    "\n",
    "#load and preprocess an audio file\n",
    "wav_file_path = \"/path/to/your/audio.wav\"\n",
    "features = audio_features(wav_file_path)\n",
    "\n",
    "#convert features to tensor and move to GPU\n",
    "features_tensor = torch.tensor(features, dtype=torch.float).to(\"cuda\")\n",
    "\n",
    "#prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(features_tensor.unsqueeze(0))  \n",
    "    predicted_label = outputs.argmax(1).item()\n",
    "\n",
    "ravdess_label_dict = {\n",
    "    \"01\": \"neutral\", \"02\": \"calm\", \"03\": \"happy\", \"04\": \"sad\",\n",
    "    \"05\": \"angry\", \"06\": \"fear\", \"07\": \"disgust\", \"08\": \"surprise\"\n",
    "}\n",
    "predicted_emotion = list(ravdess_label_dict.values())[predicted_label]\n",
    "\n",
    "print(f\"Predicted emotion: {predicted_emotion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2034e2fc-35bd-4bfe-bb6e-e230f63692d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
